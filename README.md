# ระบบเพิ่มขนาดชุดข้อมูล (Data Augmentation System)

ระบบเพิ่มขนาดชุดข้อมูลที่มีประสิทธิภาพ พร้อมการประมวลผลแบบขนาน การวิเคราะห์เชิงลึก และส่วนติดต่อผู้ใช้ที่ใช้งานง่าย

## คุณสมบัติหลัก

### การรับข้อมูล
- รองรับหลายรูปแบบไฟล์:
  - ไฟล์ข้อความทั่วไป (Plain text)
  - รูปแบบ CoNLL (สำหรับงาน NER)
  - รูปแบบ JSON
- ระบบการจัดการแหล่งข้อมูลที่ยืดหยุ่น สามารถเพิ่มแหล่งข้อมูลใหม่ได้ง่าย

### เทคนิคการเพิ่มขนาดข้อมูล
#### การเพิ่มขนาดข้อความ
- การแทนที่ด้วยคำพ้องความหมาย (Synonym Replacement)
  - ใช้ WordNet ในการค้นหาคำที่มีความหมายใกล้เคียง
  - สามารถกำหนดความน่าจะเป็นในการแทนที่
- การแทรกคำแบบสุ่ม (Random Insertion)
  - แทรกคำที่มีความหมายเกี่ยวข้องในตำแหน่งที่สุ่ม
  - ควบคุมจำนวนคำที่จะแทรกได้
- การลบคำแบบสุ่ม (Random Deletion)
  - ลบคำที่ไม่สำคัญออกแบบสุ่ม
  - รักษาความหมายหลักของประโยคไว้
- การสลับตำแหน่งคำ (Random Swap)
  - สลับตำแหน่งคำในประโยคแบบสุ่ม
  - รักษาโครงสร้างทางไวยากรณ์

### การวิเคราะห์และรายงานผล
- สถิติของชุดข้อมูล:
  - ความหลากหลายของคำศัพท์ (Vocabulary Diversity)
  - การวิเคราะห์ความสมดุลของคลาส (Class Balance)
  - การตรวจจับความซ้ำซ้อน (Redundancy Detection)
- การแสดงผลแบบโต้ตอบ:
  - กราฟแสดงความถี่ของคำ
  - การกระจายตัวของความยาวประโยค
  - การแสดงผลแบบ Word Cloud
  - กราฟแสดงการกระจายตัวของคลาส

## การติดตั้ง

1. โคลนโปรเจค:
```bash
git clone [repository-url]
cd data-augmentation-system
```

2. สร้าง virtual environment (แนะนำ):
```bash
python -m venv venv
source venv/bin/activate  # สำหรับ Windows ใช้: venv\Scripts\activate
```

3. ติดตั้ง dependencies:
```bash
pip install -r requirements.txt
```

4. ดาวน์โหลดข้อมูล NLTK ที่จำเป็น:
```python
python -c "import nltk; nltk.download('wordnet'); nltk.download('punkt'); nltk.download('averaged_perceptron_tagger')"
```

## วิธีการใช้งาน

### 1. เริ่มต้นใช้งานระบบ
```bash
streamlit run main.py
```

### 2. การใช้งานผ่านเว็บอินเตอร์เฟซ

#### การอัพโหลดข้อมูล
1. คลิกที่แท็บ "Upload"
2. เลือกไฟล์ข้อมูลของคุณ (รองรับ .txt, .csv, .json)
3. เลือกรูปแบบข้อมูล (Plain Text, CoNLL, JSON)
4. ระบบจะแสดงตัวอย่างข้อมูลที่อัพโหลด

#### การตั้งค่าการเพิ่มขนาดข้อมูล
1. ไปที่แท็บ "Augment"
2. ปรับแต่งพารามิเตอร์:
   - synonym_replace_prob: ความน่าจะเป็นในการแทนที่คำด้วยคำพ้องความหมาย (0-1)
   - random_insert_prob: ความน่าจะเป็นในการแทรกคำ (0-1)
   - random_delete_prob: ความน่าจะเป็นในการลบคำ (0-1)
   - random_swap_prob: ความน่าจะเป็นในการสลับคำ (0-1)
   - max_augmentations: จำนวนตัวอย่างที่ต้องการสร้าง
3. คลิก "Generate Preview" เพื่อดูตัวอย่างผลลัพธ์
4. คลิก "Augment Data" เพื่อสร้างข้อมูลทั้งหมด

#### การวิเคราะห์ข้อมูล
1. ไปที่แท็บ "Analyze"
2. เลือกข้อมูลที่ต้องการวิเคราะห์ (ข้อมูลต้นฉบับหรือข้อมูลที่เพิ่มขนาดแล้ว)
3. คลิก "Analyze" เพื่อดูผลการวิเคราะห์:
   - ขนาดคำศัพท์
   - ความยาวเฉลี่ยของประโยค
   - อัตราส่วน Type-Token
   - จำนวนคำที่ปรากฏครั้งเดียว
   - อัตราความซ้ำซ้อน
   - กราฟและการแสดงผลต่างๆ

#### การดาวน์โหลดผลลัพธ์
1. ไปที่แท็บ "Download"
2. เลือกรูปแบบไฟล์ที่ต้องการ
3. คลิกปุ่มดาวน์โหลด

## หลักการทำงาน

### การเพิ่มขนาดข้อมูลแบบขนาน
- ใช้ Python multiprocessing สำหรับการประมวลผลพร้อมกัน
- แบ่งข้อมูลเป็นชุดย่อยเพื่อประสิทธิภาพสูงสุด
- รองรับการประมวลผลไฟล์ขนาดใหญ่

### การวิเคราะห์คุณภาพข้อมูล
- ตรวจสอบความหลากหลายของคำศัพท์
- วิเคราะห์โครงสร้างประโยค
- ตรวจจับความซ้ำซ้อนโดยใช้ TF-IDF และ Cosine Similarity
- วิเคราะห์ความสมดุลของคลาสสำหรับงาน Classification

### การจัดการหน่วยความจำ
- ใช้ temporary files สำหรับไฟล์ขนาดใหญ่
- จัดการหน่วยความจำอย่างมีประสิทธิภาพ
- ลบไฟล์ชั่วคราวอัตโนมัติ

## โครงสร้างโปรเจค

```
data-augmentation-system/
├── main.py                 # จุดเริ่มต้นแอปพลิเคชัน
├── requirements.txt        # รายการ dependencies
├── README.md              # เอกสารประกอบ
├── data_sources/          # โมดูลจัดการแหล่งข้อมูล
├── augmentation/          # โมดูลเพิ่มขนาดข้อมูล
├── analysis/              # โมดูลวิเคราะห์ข้อมูล
└── utils/                 # ฟังก์ชันช่วยเหลือ
```

## การพัฒนาต่อยอด

### การเพิ่มแหล่งข้อมูลใหม่
1. สร้างคลาสที่สืบทอดจาก `BaseDataSource`
2. implement เมธอดที่จำเป็น:
   - `load_data()`
   - `validate_data()`
   - `get_metadata()`
   - `get_sample()`

### การเพิ่มเทคนิคการเพิ่มขนาดข้อมูล
1. สร้างคลาสที่สืบทอดจาก `BaseAugmenter`
2. implement เมธอดที่จำเป็น:
   - `augment()`
   - `validate_params()`
   - `get_param_info()`
   - `get_description()`

### การเพิ่มวิธีการวิเคราะห์
1. สร้างคลาสที่สืบทอดจาก `BaseAnalyzer`
2. implement เมธอดที่จำเป็น:
   - `calculate_metrics()`
   - `generate_visualizations()`
   - `get_metric_descriptions()`

## การแก้ไขปัญหาที่พบบ่อย

1. **ปัญหา NLTK**
   - ตรวจสอบว่าได้ดาวน์โหลดข้อมูล NLTK ครบถ้วน
   - ลองรันคำสั่งดาวน์โหลดอีกครั้ง

2. **ปัญหาหน่วยความจำ**
   - ลดขนาดชุดข้อมูลที่โหลดพร้อมกัน
   - เพิ่มขนาด chunk size ในการประมวลผล

3. **ปัญหาการแสดงผล**
   - รีเฟรชหน้าเว็บ
   - ตรวจสอบ log ในเทอร์มินัล

## ข้อควรระวัง

1. **การเพิ่มขนาดข้อมูล**
   - ระวังการตั้งค่าพารามิเตอร์ที่สูงเกินไป
   - ตรวจสอบผลลัพธ์เสมอก่อนใช้งานจริง

2. **การใช้ทรัพยากร**
   - ระวังการใช้หน่วยความจำมากเกินไป
   - ตรวจสอบพื้นที่ดิสก์สำหรับไฟล์ชั่วคราว

3. **คุณภาพข้อมูล**
   - ตรวจสอบความถูกต้องของข้อมูลที่เพิ่มขนาด
   - ระวังการสูญเสียความหมายของประโยค

## การสนับสนุน

หากพบปัญหาหรือต้องการความช่วยเหลือ สามารถติดต่อได้ที่:
- สร้าง Issue ใน GitHub
- ส่งอีเมลมาที่ [อีเมลผู้พัฒนา]

## License

โปรเจคนี้อยู่ภายใต้ MIT License - ดูรายละเอียดเพิ่มเติมได้ที่ไฟล์ LICENSE
